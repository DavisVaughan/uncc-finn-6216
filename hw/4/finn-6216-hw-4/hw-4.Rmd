---
title: "HW 4"
author: "Davis Vaughan"
date: "2/14/2018"
output: html_document
---

# Required packages

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(lubridate)
library(tidyquant)
options(pillar.sigfig = 6)

# To install the above packages if you don't have them:
# install.packages("dplyr")
# install.packages("lubridate")
# install.packages("devtools")
# devtools::install_github("business-science/tidyquant")
```

# Question 3

### Get the data

```{r}
position_per_stock <- 1000000
num_factors <- 2
stocks <- c("F", "GM", "IBM", "MSFT", "AAPL", "AMZN", "SPY", "XOM", "PFE", "BA")

last_day <- as.Date("2018-02-02")
first_day <- last_day - years(1)

# stocks_raw <- tq_get(stocks, from = first_day, to = last_day)
# write_rds(stocks_raw, "stocks_raw.rds")
stocks_raw <- read_rds(path = "stocks_raw.rds")

stocks_prices <- stocks_raw %>%
  select(symbol, date, close)

# All of the stocks are here in a "long" format
# The symbol column specifies which row belongs to which stock
head(stocks_prices)
```

### Compute relative shifts

```{r}
stocks_shifts <- stocks_prices %>%
  
  # For each ticker
  group_by(symbol) %>%
  
  # Calculate the delta_X values
  mutate(daily_return = close / lag(close) - 1,
         delta_X  = daily_return * position_per_stock) %>%
  
  select(symbol, date, delta_X) %>%
  ungroup() %>%
  na.omit()

head(stocks_shifts)
```

### PCA

Spread the data and turn it into a matrix

```{r}
delta_X_wide <- stocks_shifts %>%
  mutate(symbol = as_factor(symbol)) %>%
  spread(symbol, delta_X) %>%
  select(-date) %>%
  as.matrix()

head(delta_X_wide)
```


```{r}
# Mean
delta_X_mean <- colMeans(delta_X_wide)

# Covariance matrix
cov(delta_X_wide - delta_X_mean)

# PCA on the centered values
pca <- princomp(delta_X_wide - delta_X_mean)

# Extract the eigenvectors of the first 2 columns
eigenvector <- pca$loadings
eigenvector_reduced <- eigenvector[, 1:num_factors]

# Summarise results
pca_results <- tibble(
  lambda = pca$sdev ^ 2,
  prop_of_variance = lambda / sum(lambda),
  cumulative_prop = cumsum(lambda) / sum(lambda)
)

pca_results
```

The loadings look like this:

```{r}
eigenvector
```


```{r, include=FALSE}
pca_first_two_printable <- paste0(round(pca_results$cumulative_prop[num_factors] * 100, 2), "%")
```

For just the first two factors, `r pca_first_two_printable` of the variability is explained.

### VaR with just two factors

```{r}
# Also calculate the 99th percentile number we will use for VaR
percentile_99_VaR <- floor(nrow(delta_X_wide) * .01)
percentile_99_VaR
```


```{r}
# Calculate Y
Y <- (delta_X_wide - delta_X_mean) %*% eigenvector_reduced

# Then use that to "predict" the in sample deltaX values
delta_X_wide_predicted <- Y %*% t(eigenvector_reduced) + delta_X_mean

# rowSum this to get portfolio values
delta_P_pca <- rowSums(delta_X_wide_predicted)

# Calc VaR
VaR_pca <- delta_P_pca %>%
  sort() %>%
  .[percentile_99_VaR] %>%
  set_names("VaR PCA Multi-Factor")

VaR_pca
```

### VaR with full reevaluation

```{r}
# Just use all the stocks
delta_P <- rowSums(delta_X_wide)

# VaR
VaR_full <- delta_P %>%
  sort() %>%
  .[percentile_99_VaR] %>%
  set_names("VaR all stocks")

VaR_full
```

### Summary

The PCA estimate of `r VaR_pca[[1]]` get's pretty close to the VaR estimate from using all 
10 stocks of `r VaR_full[[1]]`, however it does overestimate the VaR. 

# Question 4

### Get data

```{r}
portfolio <- c("AAPL", "SPY")

# Dates
end_date   <- as.Date("2018-01-05")
start_date <- end_date - years(2) + days(2) # Add two days to get the correct 504 prices

# portfolio_raw  <- tq_get(portfolio,  from = start_date, to = end_date, get = "stock.prices.google")
# write_rds(portfolio_raw, "portfolio_raw.rds")
portfolio_raw <- read_rds("portfolio_raw.rds")
```

### Calculate shifts

```{r}
portfolio_shifts <- portfolio_raw %>%
  select(symbol, date, close) %>%
  group_by(symbol) %>%
  mutate(delta_X = close - lag(close)) %>%
  na.omit()
```

### Spread data

```{r}
portfolio_wide <- portfolio_shifts %>%
  select(-close) %>%
  spread(symbol, delta_X) %>%
  select(-date) %>%
  as.matrix()
```

### Elliptical fit functions

```{r}
rowwise_d2 <- function(row, mu_k, cov_k) {
  d <- length(row)
  row <- matrix(row, ncol = d)
  (row - mu_k) %*% solve(cov_k) %*% t(row - mu_k)
}

weighter <- function(x, cutoff) {
  becomes_1 <- x < cutoff
  weighted <- cutoff / x
  weighted[becomes_1] <- 1
  weighted
}

elliptical_fitter <- function(x) {
  
  d <- ncol(x)
  x_mean_k <- colMeans(x)
  x_cov_k <- cov(x)
  
  for(k in 1:1000) {
    
    d2 <- apply(x, 1, rowwise_d2, mu_k = x_mean_k, cov_k = x_cov_k)
    d <- sqrt(d2)
    
    # Weight d's with arbitrary cutoffs
    d_weighted  <- weighter(d, 2)
    d2_weighted <- weighter(d2, 2)
    
    # Update covariance (happens before mean update)
    x_cov_k_updated <- (t(d2_weighted * (x - x_mean_k)) %*% (x - x_mean_k) ) / sum(d2_weighted)
    
    # Update mean
    x_mean_k_updated <- colSums(d_weighted * x) / sum(d_weighted)
    
    # Norms for comparison
    x_mean_k_updated_norm <- norm(as.matrix(x_mean_k_updated), type = "F")
    x_mean_k_norm         <- norm(as.matrix(x_mean_k), type = "F")
    x_cov_k_updated_norm  <- norm(as.matrix(x_cov_k_updated), type = "F")
    x_cov_k_norm          <- norm(as.matrix(x_cov_k), type = "F")
    
    if(abs(x_mean_k_updated_norm - x_mean_k_norm) <= 0.0000001 && abs(x_cov_k_updated_norm - x_cov_k_norm) <= 0.0000001) {
      
      break()
      
    } else {
      
      x_mean_k <- x_mean_k_updated
      x_cov_k  <- x_cov_k_updated
      
    }
    
    #message(paste("Finishing iteration", k, "with mean =", x_mean_k[1], x_mean_k[2]))
  }
  
  list(mean = x_mean_k_updated, cov = x_cov_k_updated)
}
```

### Run the fitter

```{r}
optimal_elliptical_coef <- elliptical_fitter(portfolio_wide)
optimal_elliptical_coef
```

### Elliptical test

```{r}
optimal_d2 <- apply(portfolio_wide, 1, rowwise_d2, mu_k = optimal_elliptical_coef$mean, cov_k = optimal_elliptical_coef$cov)

num_c_vals <- 10

# Setup test tibble
elliptical_test <- tibble(
  c = rep(0:(num_c_vals-1), each = length(optimal_d2)),
  d2 = rep(optimal_d2, times = num_c_vals)
) %>%
  bind_cols(do.call(bind_rows, replicate(num_c_vals, portfolio_wide %>% as.tibble(), simplify = FALSE)))


elliptical_test %>%
  
  # For each c...
  group_by(c) %>%
  
  # Add the probability columns
  mutate(
    greater_than_c = d2 > c,
    AAPL_given_d2  = if_else(greater_than_c, AAPL, 0),
    SPY_given_d2   = if_else(greater_than_c, SPY, 0)
  ) %>%
  
  # Summarise the correlations
  summarise(
    any_greater_than_c = sum(greater_than_c),
    corr               = cor(AAPL_given_d2, SPY_given_d2)
  )

# Also check against the original
cor(portfolio_wide)
```

### Summary

The correlations of `X` and `X given D^2 > c` are roughly the same (especially for
low c values). I would conclude that they are elliptical.



